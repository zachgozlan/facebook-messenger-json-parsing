{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by: Zach Gozlan**  \n",
    "McCourt School of Public Policy  \n",
    "MS-DSPP, May 2020 Grad  \n",
    "September 8, 2019  \n",
    "\n",
    "These are a series of tools that I created for analyzing Facebook messenger conversations; I wrote these over the course of Summer 2019 prior to attending a pair of weddings. At both weddings, I had substantial Messenger conversations with one side of the couple. At one, I used information from this process to inform a speech I gave; at the other, the findings were presented as a 15-page \"card.\"\n",
    "\n",
    "To download your own data for this analysis, the path is currently Settings -> Your Facebook Information -> Download Your Information; select \"json\" as the output. As these files can be massive (I have been on Facebook since 2007) I strongly suggest filtering down to just \"messages\" on the selection screen and cutting dates down if you're aware of the earliest date the conversation could've began at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: Packages, Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "\n",
    "import pandas as pd #dataframe package\n",
    "import os           #for navigating xml library\n",
    "import json         #for navigating json\n",
    "#import ijson        #same\n",
    "from pandas.io.json import json_normalize #same\n",
    "import numpy as np  #habit\n",
    "import networkx     #network package\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import requests     #for twitter api adding\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "#import tweepy #twitter api stuff\n",
    "import random\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "\n",
    "\n",
    "#I'm pretty sure I use all of these, and import statements are basically costless so whatever dude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword list, probably based on https://gist.github.com/sebleier/554280 but added a few words that come up a lot\n",
    "\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", 'nickname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your file here; as far as I can tell it's always message_1 within the nested folder referencing the conversation\n",
    "\n",
    "data = pd.read_json('personal projects/message_1.json', typ='series', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#echo back to see the data frame makes sense\n",
    "\n",
    "df = pd.DataFrame.from_dict(data.messages, orient='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting timestamps to useable format\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='ms')\n",
    "df.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sender_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word counts across all messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "df2 = df[df['type'] == 'Generic'] #removes certain automated messages not sent by user\n",
    "\n",
    "for sentence in df2['content']:\n",
    "    if type(sentence) is not float:\n",
    "        counts.update(word.strip('.,?!\"/:@;@\\)$/-(][').lower() for word in sentence.split())\n",
    "    \n",
    "for item in stopwords: \n",
    "    del counts[item]\n",
    "    \n",
    "word_count_dict = pd.DataFrame.from_dict(counts, orient='index')\n",
    "word_count_dict = word_count_dict.sort_values(0, ascending=False)\n",
    "word_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timestamps of specific phrases; case sensitive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.dropna(subset=['content'])\n",
    "df3.loc[df3.content.str.contains('good morning'), 'timestamp'] #replace 'good morning' with your word or phrase of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All YouTube links shared**\n",
    "\n",
    "With some processing and when combined with https://www.labnol.org/internet/youtube-playlist-spreadsheet/29183/, this can be used to quickly work through all videos shared without having to click through all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['share']\n",
    "youtube = []\n",
    "for item in messages:\n",
    "    if 'youtube' in str(item):\n",
    "        youtube.append(item)\n",
    "        \n",
    "DFyoutube = pd.DataFrame({'col':youtube})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFyoutube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Message Reaction Information**\n",
    "\n",
    "You know, like, thumbs up, thumbs down, heart, that stuff. It was added to Messenger relatively recently so older conversations may not include this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions = pd.DataFrame.from_dict(df['reactions'], orient='columns')\n",
    "reactions['content'] = df['content']\n",
    "\n",
    "x = pd.DataFrame(reactions['reactions'].apply(pd.Series))\n",
    "list(x.columns) #the number of columns on x is equal to the maximum number of reactions received by any message in the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There must be one 'reactions['r1']' column for every column in the output list(x.columns). By default I am \n",
    "#treating this like a 1-on-1 conversation, but I have included example code for up to seven.\n",
    "\n",
    "reactions['r1'] = x[0]\n",
    "#reactions['r1'], reactions['r2'], reactions['r3'], reactions['r4'], reactions['r5'], reactions['r6'], reactions['r7'] = x[0], x[1], x[2], x[3], x[4], x[5], x[6]\n",
    "\n",
    "reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions['r1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses reactions into separate columns for the sender and what they sent\n",
    "#very inefficient and i'd be totally down for fixes on this\n",
    "\n",
    "y = reactions['r1'][6]\n",
    "\n",
    "#y['reaction']\n",
    "\n",
    "columns = ['r1'] #'r2'] #, 'r3', 'r4', 'r5', 'r6', 'r7']\n",
    "\n",
    "for item in columns:\n",
    "    print(item)\n",
    "    head_r = str(item) + '_reactions'\n",
    "    head_a = str(item) + '_actor'\n",
    "    list_1 = reactions[item].apply(pd.Series)['reaction']\n",
    "    list_2 = reactions[item].apply(pd.Series)['actor']\n",
    "    reactions[head_r] = list_1\n",
    "    reactions[head_a] = list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes a while and I recommend saving the output when it's complete\n",
    "#reactions = pd.read_csv('personal projects/reactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement encoded emoji names with their intuitive meanings and join output reaction columns back to main dataframe \n",
    "\n",
    "reactions2 = reactions[['r1_reactions', 'r1_actor']]\n",
    "\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x98\\x8d', 'heart')\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x91\\x8d', 'thumbsup')\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x98\\x86', 'laugh')\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x98®', 'wow')\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x98\\xa0', 'angry')\n",
    "reactions2 = reactions2.replace('ð\\x9f\\x98¢', 'sad')\n",
    "reactions2 =reactions2.replace('ð\\x9f\\x91\\x8e', 'thumbsdown')\n",
    "reactions2 = reactions2.replace('â¤', 'other')\n",
    "\n",
    "#EXAMPLE CODE FOR GROUP CHATS:\n",
    "#reactions2 = reactions[['r1_reactions', 'r1_actor', 'r2_reactions', 'r2_actor', 'r3_reactions',\n",
    "#       'r3_actor', 'r4_reactions', 'r4_actor', 'r5_reactions', 'r5_actor',\n",
    "#       'r6_reactions', 'r6_actor', 'r7_reactions', 'r7_actor']]\n",
    "df = df.join(reactions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Unique Terms by Sender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spare = df[['sender_name', 'content']]\n",
    "df_spare = pd.concat([df_spare, pd.get_dummies(df_spare['sender_name'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_array_full = np.asarray(df_spare['Zach GozÅan'])\n",
    "target_array_full = np.asarray(df_spare['Beth Shobudubudub'])\n",
    "corpus = [str(item).lower() for item in df_spare['content']]\n",
    "\n",
    "tfid_max = int(len(df_spare['content'])*.9)\n",
    "tfid_vectorizer_all = TfidfVectorizer(ngram_range=(1,1), min_df=10, max_df=tfid_max, stop_words=stopwords)\n",
    "full_corpus_vectorizer = tfid_vectorizer_all.fit(corpus)\n",
    "feature_matrix = tfid_vectorizer_all.transform(corpus)\n",
    "\n",
    "print(feature_matrix.shape)\n",
    "print(target_array_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing a straightforward linear regression here but feel free to try something more complex\n",
    "\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(feature_matrix, target_array_full)\n",
    "coefficients = list(lreg.coef_)\n",
    "\n",
    "feature_names = tfid_vectorizer_all.get_feature_names()\n",
    "df = pd.DataFrame(list(zip(feature_names, coefficients)), \n",
    "               columns =['Feature', 'Coefficient']) \n",
    "df_nonzero = df[df['Coefficient'] != 0].sort_values('Coefficient', ascending=False) #only shows non-zero coefficients\n",
    "df_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
